{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96698930",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "373e548a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (4.53.2)\n",
      "Requirement already satisfied: datasets in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: torchaudio in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: accelerate in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: filelock in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: xxhash in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: torch==2.7.1 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from torchaudio) (2.7.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from torch==2.7.1->torchaudio) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from torch==2.7.1->torchaudio) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from torch==2.7.1->torchaudio) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from torch==2.7.1->torchaudio) (80.9.0)\n",
      "Requirement already satisfied: psutil in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.1->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from jinja2->torch==2.7.1->torchaudio) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\clone repo\\nvidia-speech-to-text-inference-playground\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torchaudio accelerate --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d32233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Clone Repo\\Nvidia-Speech-to-Text-Inference-Playground\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39081d2f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66feaaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deteksi device dan precision\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Model ID\n",
    "model_id = \"openai/whisper-large-v3-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8d0f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load model dan processor\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Buat pipeline ASR\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e0fe4d",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "648b0996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n",
      "c:\\Clone Repo\\Nvidia-Speech-to-Text-Inference-Playground\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:604: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Transcribing with timestamps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hasil Transkripsi:\n",
      "üïí    0.0s -   25.2s: I want to show you two video clips. One of them is real and one of them is fake. Look at these two clips. One of them is real and one of them is fake. Can you tell which one it is? Living at the dangers of AI. About AI. Artificial intelligence. The threat AI poses to the social. I've never seen it quite like this. This technology is spreading rapidly. It's really mind-blowing. Deep fakes. Deep fakes.\n",
      "üïí   25.2s -  28.46s: Deep Tom Cruise was a tipping point for deep fakes.\n",
      "üïí  28.46s -  31.88s: We're increasingly in a world where AI is everywhere.\n",
      "üïí  31.88s -  35.08s: But do we actually know what's really going on?\n",
      "üïí  35.08s -  37.24s: Let's dig into what AI really is with me,\n",
      "üïí  37.24s -  105.6s: Krystal Wijaya at Klas Bakar. Yeah, AI has actually been around for many years now. Coming from the tech field, I started doing machine learning with a Python script in my laptop maybe 20 years ago. But today, we're increasingly exposed to AI in the wild, in the wild being things like in our news, on Twitter, social media. We have to ask ourselves, is this post real or did someone deep fake this? And the paradigm is moving so quickly that it's hard to tell what we can trust anymore. Coming from tech, I know how AI works. I know when to be suspicious. When we see AI now controlling things like spam or fraud and risk, we start to see AI doing scarier things.\n",
      "üïí 105.96s - 127.36s: And so I like to deepen, to dive into what are the actual mechanics behind AI. AI at its front is really just machine learning applied. Machine learning has the ability to create reason and\n",
      "üïí 127.36s - 135.04s: infer and listen to data around it and what comes out is the ability to create new journalistic\n",
      "üïí 135.04s - 141.68s: interviews like this our scripts might be researched by ai or you might see photos that\n",
      "üïí 141.68s - 246.48s: never actually existed but were created by a generative AI product. And that might be scary. We see companies cutting jobs and saying, you can be replaced by AI. Klarna for example in the US replaced its customer service collars with AI service agents and that can be kind of brutal. We find that you no longer have that human touch anymore. I personally don't think that journalists will be the easiest entity to replace with AI. I think journalism is a media format that requires trust. And frankly, if you think about it, most people don't trust other people. And so why would humans trust machines? Well, if you think about it, AI is the first kind of machine where you've had to doubt what it says. Usually when we're given access to a computer, we can trust that it's going to do what it says it will do because it's been programmed with static data, static information, and frameworks. So we never had to ask ourselves could the resulting answer be false? But now with generative AI we do have to ask that question. We have to be a bit skeptical about about the results that we are getting from an AI program or an AI that's generating information. And so with journalism, well, at the heart of it, it's a human experience that you have to trust. You have to trust that someone has gone through the facts, has talked to other people on the ground, and picked up on data that isn't really on a\n",
      "üïí 246.48s - 250.88s: spreadsheet. It's not really available in a poll. It's something that you have to\n",
      "üïí 250.88s - 255.36s: talk to humans and ask them how they're feeling about the situation. So\n",
      "üïí 255.36s - 259.14s: journalism, while on its face, you can replace maybe some of the content. You\n",
      "üïí 259.14s - 263.68s: will not get to a place where you have trustworthy AI and that's something I\n",
      "üïí 263.68s - 505.96s: think will never change. In tech we might see companies replacing workers who are able to do simple fact retrieval. So in the legal field you may have people who are able to pull together cases but you aren't going to replace the person who has to pour through all of the details and deeply understand it. And in fact the kind of jobs that are most safe from AI replacing them are jobs where you have to interact with another human being or need to have built trust and empathy with the users. In tech you might want you might be able to replace an engineer in tech with AI because they've learned to code on the same machines, but how do you replace someone who knows how to sing or dance or create art that is innovative and new? That's not something that AI has the ability to do right now. so i'll talk about the there's a maturity with AI and how we are experiencing it in the real world today. If you think about a product like Google Maps, where initially you use it to get a static result from point A to point B, what turns do I make and how long will it take? That used to be a static snapshot from data based on, you know, geographic maps and the static laws of the streets. And then we moved into more predictive data where you were able to consider, you know, an accident might have just happened. And so the map updates itself based on new inputs of data. That would be a reactive data system, or a product that, based on a new input, is able to correct its advice. And then we move into generative and predictive mapping, where in Google Maps it knows that there isn't any traffic right now on the way home from work, but it does know based on historical data that within 30 minutes there will be about a 10 minute slowdown and so it adds that into the prediction of how long your route will take to get home before it actually happens and that ends up being an example of predictive AI. Not all companies have this ability. Not all companies should be embracing AI. We should, like many other things, embrace AI only when it makes sense for our business or for our company's needs. Most companies don't even use the data that they already have today. And so I'll talk to companies who, you know, they understand that users are churning or they're leaving their product, but they don't understand the inputs as to why. And instead, they want a very fancy AI machine learning model to tell them who's going to churn. And I say, great, I'll tell you, this customer right here, they're going to churn. And I know that because, you know, 11% of customers churn on the first day. What are you going to do about it? And they have no answer. And so AI, when used to make predictions without telling you the reasons why it's made that prediction, in the case of most deep learning, isn't that useful. It might help you build a financial model, but it doesn't help you act in the real world. So there are many companies today that survive not because they have the most data or the cleanest data but because they are a delightful product and delight is rare today you need people to understand and have empathy to have one there's the offline services a therapist for someone to kind of the emotion with you, to hold your hand with you as you are crying through a bad day. That's not something AI is going to be able to do and if we make a robot out of it I'm not sure that I'm going to want that hug versus you know my parents or a friend. A shoulder to cry, is not going to be replaced by AI.\n",
      "üïí 506.74s - 513.98s: We may make tech-enabled tools that maybe use AI to create better and faster services or matchmaking,\n",
      "üïí 514.36s - 517.72s: but you're not going to have a product whose delightfulness,\n",
      "üïí  518.2s -  521.7s: it's the ability to scroll and see animations,\n",
      "üïí  521.7s -  570.8s: the ability to create characters that you love, that are mascots of your app. Think about the last time you scrolled Instagram and you thought to yourself, wow, that's a really cool post, that's a really cool visual. That probably wasn't AI-generated, it was probably someone's taste. And taste is something you can't magically create through data. Most people don't have good taste. And as a result, AI is pretty bad taste. It's quite mid. And so if you want to be someone who is irreplaceable, cannot be replaced by AI, you've got to have really good taste. You'll have to be designed first. That requires human empathy, an ability to understand how the world\n",
      "üïí  570.8s - 575.78s: perceives the products and services that we have to offer, and to make something\n",
      "üïí 575.78s - 578.12s: compelling.\n",
      "üïí 581.06s - 586.04s: For those of us who are in engineering today or maybe thought, you know, it's a\n",
      "üïí 586.04s - 589.94s: good time to become an engineer, it's still true. It's still true that we need\n",
      "üïí 589.94s -  626.0s: Thank you. For those of us who are in engineering today or maybe thought, you know, it's a good time to become an engineer, it's still true. It's still true that we need more engineers. It's just that the bar for an engineer has gone a lot higher. It's no longer enough to be able to code or know a library. It's going to be important to be someone who makes thoughtful decisions, that can understand and perceive how the business is moving, what architectural design decisions have been made, and the types of people at the company, whether or not they might like this type of library versus another one. There's never been a better time to be an excellent engineer is what I tell people. It's a pretty bad time to be an engineer who can only write code,\n",
      "üïí  626.0s -  630.0s: and instead will have to care a lot more about the business.\n",
      "üïí  630.0s -  634.0s: Engineers who only care about code and tech\n",
      "üïí  634.0s -  637.0s: and don't look beyond their scope of work\n",
      "üïí  637.0s -  639.0s: are going to have a really bad time in a world\n",
      "üïí  639.0s -  643.0s: where AI can replace a lot of the functional tasks\n",
      "üïí  643.0s - 666.76s: that we used to do as engineers. And instead we'll have to do what AI cannot do, which is talk to our customers. Try to design a compelling experience to consider and make decisions on how people will experience our product and that can be as\n",
      "üïí 666.76s - 672.32s: simple as the vibe that you want your landing page to look like is a decision\n",
      "üïí 672.32s - 677.86s: that most AI leaves up to chance and if you leave things up to chance you are no\n",
      "üïí 677.86s - 681.02s: better than an AI system.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "audio_path = \"Kamu Harus Punya Hal Ini Biar Gak Diganti AI_processed.wav\"\n",
    "\n",
    "# Librosa automatically resamples to 16kHz and mono\n",
    "waveform, sample_rate = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "# Format untuk pipeline Hugging Face\n",
    "sample = {\"array\": waveform, \"sampling_rate\": sample_rate}\n",
    "\n",
    "# Run transcription with timestamps\n",
    "print(\"‚è≥ Transcribing with timestamps...\")\n",
    "\n",
    "result = pipe(\n",
    "    sample,\n",
    "    return_timestamps=True,       # <- Enable timestamp output\n",
    "    chunk_length_s=30,            # <- Optional: force chunking to 30s per segment\n",
    "    stride_length_s=(5, 5),       # <- Overlap between chunks\n",
    ")\n",
    "\n",
    "# Print result\n",
    "print(\"‚úÖ Hasil Transkripsi:\")\n",
    "segments = result[\"chunks\"]\n",
    "for seg in segments:\n",
    "    start = round(seg[\"timestamp\"][0], 2)\n",
    "    end = round(seg[\"timestamp\"][1], 2)\n",
    "    text = seg[\"text\"].strip()\n",
    "    print(f\"üïí {start:>6}s - {end:>6}s: {text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
